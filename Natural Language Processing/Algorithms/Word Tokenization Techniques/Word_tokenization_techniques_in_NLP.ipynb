{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Word Tokenization Techniques in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is word tokenization?\n",
    "Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called **tokens**. These tokens help in understanding the context or developing the model for the NLP. \n",
    "The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the text “It is raining” can be tokenized into ‘It’, ‘is’, ‘raining’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization can be done to either separate words or sentences.\n",
    "- If the text is split into words using some separation technique it is called **word tokenization** and same separation done for sentences is called **sentence tokenization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'everyone', '.', 'Welcome', 'to', 'my', 'Notebook']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple Example of word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#input a text\n",
    "text = \"Hello everyone. Welcome to my Notebook\"\n",
    "word_tokenize(text)  #print tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to tokenize multiple sentences first, and then the words contained in sentences of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.']\n",
      "['Welcome', 'to', 'the', 'NLP', 'Class', '.']\n",
      "['Mr.Smith', 'and', 'Johann', 'S.', 'Baech', 'are', 'waiting', 'for', 'you', '.']\n",
      "['They', \"'ll\", 'join', 'you', 'soon', '.']\n"
     ]
    }
   ],
   "source": [
    "#import sent_tokenize from nltk library\n",
    "from nltk import sent_tokenize \n",
    "text = \"Hello everyone. Welcome to the NLP Class. Mr.Smith  and Johann S. Baech are waiting for you. They'll join you soon.\"\n",
    "for t in sent_tokenize(text):\n",
    "    \n",
    "    x =word_tokenize(t)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words : \n",
    "Stop words are those words in the text which does not add any meaning to the sentence and their removal will not affect the processing of text for the defined purpose. They are removed from the vocabulary to reduce noise and to reduce the dimension of the feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different libraries to perform word tokenization :\n",
    " - NLTK\n",
    " - Genism\n",
    " - Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use only nltk library in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various Tokenization Techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/945/1*FTEu803GEsNrNslvY1RbXQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Whitespace Tokenization\n",
    "This is the simplest tokenization technique. Given a sentence or paragraph it tokenizes into words by splitting the input whenever a white space in encountered. This is the fastest tokenization technique but will work for languages in which the white space breaks apart the sentence into meaningful words.\n",
    "#### Syntax : tokenize.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also import WhitespaceTokenizer() method from nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'Processing', 'is', 'a', 'subset', 'of', 'Deep', 'learning.']\n"
     ]
    }
   ],
   "source": [
    "# Create a reference variable for Class WhitespaceTokenizer\n",
    "wtk = WhitespaceTokenizer()\n",
    "\n",
    "#give string input\n",
    "text1 = \"Natural language Processing is a subset of Deep learning.\"\n",
    "\n",
    "#use tokenize method\n",
    "tokens = wtk.tokenize(text1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Dictionary Based Tokenization\n",
    "In this method the tokens are found based on the tokens already existing in the dictionary. If the token is not found, then special rules are used to tokenize it. It is an advanced technique compared to whitespace tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Vegetable'], ['Fruit'], ['Dry-fruit']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list and dictionary variable\n",
    "lst = ['Tomato', 'Orange','Almond']\n",
    "dct = {'Vegetable': 'Tomato', 'Fruit': 'Orange','Dry-fruit':'Almond'}\n",
    "\n",
    "#extract words from dictonary items\n",
    "word2index = {key: val for val, key in dct.items()}\n",
    "\n",
    "#print tokenized words\n",
    "tokenized = [[word2index[word] for word in text.split()] for text in lst]\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Rule Based Tokenization\n",
    "In this technique a set of rules are created for the specific problem. The tokenization is done based on the rules. For example creating rules bases on grammar for particular language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression Tokenizer\n",
    "This technique uses regular expression to control the tokenization of text into tokens. Regular expression can be simple to complex and sometimes difficult to comprehend. This technique should be preferred when the above methods does not serve the required purpose. **It is a rule based tokenizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's\", 'see', 'how', \"it's\", 'working']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import RegexpTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "  \n",
    "tk = RegexpTokenizer(\"[\\w']+\")  #[\\w']+ is one type of regular expression which extracts whole words from text.\n",
    "\n",
    "#give an input string\n",
    "text = \"Let's see how it's working.\"\n",
    "tokens = tk.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are numerous Regular Expressions from which we can choose according to our requirements.\n",
    "![](https://www.umbrella-host.co.uk/wp/media/Regular-Expressions-Cheat-Sheet-800x364.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is a complete [Tutorial](https://pycon2016.regex.training/regex-intro) on Regular expression for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation-based tokenizer\n",
    "Punctuation-based tokenization splits on whitespace and punctuations and also retains the punctuations.Punctuation-based tokenization overcomes the issue above and provides a meaningful token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mrs',\n",
       " '.',\n",
       " 'Patel',\n",
       " 'buys',\n",
       " 'Fruits',\n",
       " ':',\n",
       " 'Mango',\n",
       " ',',\n",
       " 'Banana',\n",
       " ',',\n",
       " 'Orange',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import wordpunct_tokenize from nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text = \"Mrs.Patel buys Fruits : Mango,Banana,Orange. \"\n",
    "\n",
    "tokens = wordpunct_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Tokenizer\n",
    "Special texts, like Twitter tweets, have a characteristic structure and the generic tokenizers mentioned above fail to produce viable tokens when applied to these datasets. NLTK offers a special tokenizer for tweets to help in this case. This is a **rule-based tokenizer** that can remove HTML code, remove problematic characters, remove Twitter handles, and normalize text length by reducing the occurrence of repeated letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', 'NLP', 'is', 'way', 'tooo', 'coool', ':-)', ':-P', '<3']\n"
     ]
    }
   ],
   "source": [
    "#import TweetTokenizer from nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#create object of tokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "tweet= \" @NLP_learner: NLP is way tooo coool:-) :-P <3\"\n",
    "\n",
    "x= tknzr.tokenize(tweet)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWE(Multi-Word Expression) Tokenizer\n",
    "The multi-word expression tokenizer is a rule-based, “add-on” tokenizer offered by NLTK. Once the text has been tokenized by a tokenizer of choice, some tokens can be re-grouped into multi-word expressions.\n",
    "- MWETokenizer takes a string and merges multi-word expressions into single tokens, using a lexicon of MWEs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'M_W_E', 'in', 'Natural_Language_Processing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "   \n",
    "# Create a reference variable for Class MWETokenizer\n",
    "tk = MWETokenizer([('M', 'W', 'E'), ('Multi', 'Word', 'Tokenier')])\n",
    "tk.add_mwe(('Natural', 'Language', 'Processing'))\n",
    "   \n",
    "# Create a string input\n",
    "text = \"What is M W E in Natural Language Processing\"\n",
    "   \n",
    "# Use tokenize method\n",
    "tokenized = tk.tokenize(text.split())\n",
    "   \n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)Penn TreeBank/Default Tokenization\n",
    "Tree bank is a corpus created which gives the semantic and syntactical annotation of language. Penn Treebank is one of the largest treebanks which was published. This technique of tokenization separates the punctuation, clitics (words that occur along with other words like I’m, don’t) and hyphenated words together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', \"'s\", 'True', ',', 'Mrs.Samantha', 'Jones', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import tokenizer from nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "#create object of TreebankWordTokenizer\n",
    "tk = TreebankWordTokenizer()\n",
    "text = \"That's True, Mrs.Samantha Jones.\"\n",
    "tokens = tk.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Spacy Tokenizer\n",
    "This is a modern technique of tokenization which faster and easily customizable. It provides the flexibility to specify special tokens that need not be segmented or need to be segmented using special rules. Suppose you want to keep $ as a separate token, it takes precedence over other tokenization operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors by creating object\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give an input string with multiple sentences\n",
    "string = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Founded',\n",
       " 'in',\n",
       " '2002',\n",
       " ',',\n",
       " 'SpaceX',\n",
       " '’s',\n",
       " 'mission',\n",
       " 'is',\n",
       " 'to',\n",
       " 'enable',\n",
       " 'humans',\n",
       " 'to',\n",
       " 'become',\n",
       " 'a',\n",
       " 'spacefaring',\n",
       " 'civilization',\n",
       " 'and',\n",
       " 'a',\n",
       " 'multi',\n",
       " '-',\n",
       " 'planet',\n",
       " '\\n',\n",
       " 'species',\n",
       " 'by',\n",
       " 'building',\n",
       " 'a',\n",
       " 'self',\n",
       " '-',\n",
       " 'sustaining',\n",
       " 'city',\n",
       " 'on',\n",
       " 'Mars',\n",
       " '.',\n",
       " 'In',\n",
       " '2008',\n",
       " ',',\n",
       " 'SpaceX',\n",
       " '’s',\n",
       " 'Falcon',\n",
       " '1',\n",
       " 'became',\n",
       " 'the',\n",
       " 'first',\n",
       " 'privately',\n",
       " 'developed',\n",
       " '\\n',\n",
       " 'liquid',\n",
       " '-',\n",
       " 'fuel',\n",
       " 'launch',\n",
       " 'vehicle',\n",
       " 'to',\n",
       " 'orbit',\n",
       " 'the',\n",
       " 'Earth',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nlp(string)\n",
    "\n",
    "# Create list of word tokens\n",
    "token_list = []\n",
    "for token in text:\n",
    "    token_list.append(token.text)\n",
    "token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Moses Tokenizer\n",
    "This is a tokenizer which is advanced and is available before Spacy was introduced. It is basically a collection of complex normalization and segmentation logic which works very well for structured language like English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sacremoses\n",
    "from sacremoses import MosesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['okay', ',', 'Let', 'me', 'check', 'Moses', 'Tokenizer', ',', 'Mr.Kim']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create tokenizer object\n",
    "mt = MosesTokenizer()\n",
    "text = \"okay,Let me check Moses Tokenizer, Mr.Kim\"\n",
    "tokens = mt.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Subword Tokenization\n",
    "This tokenization is very useful for specific application where sub words make significance. In this technique the most frequently used words are given unique ids and less frequent words are split into sub words and they best represent the meaning independently. For example if the word few is appearing frequently in the text it will be assigned a unique id, where fewer and fewest which are rare words and are less frequent in the text will be split into sub words like few, er, and est. This helps the language model not to learn fewer and fewest as two separate words. This allows to identify the unknown words in the data set during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> There are different types of subword tokenization and they are given below:\n",
    "\n",
    "- Byte-Pair Encoding (BPE)\n",
    "- WordPiece\n",
    "- Unigram Language Model\n",
    "- SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Byte-Pair Encoding(BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE uses Huffman encoding for tokenization meaning it uses more embedding or symbols for representing less frequent words and less symbols or embedding for more frequently used words.\n",
    "The BPE tokenization is bottom up sub word tokenization technique. The steps involved in BPE algorithm is given below.\n",
    "1. Split the words in the corpus into characters after appending </w>\n",
    "2. Initialize the vocabulary with unique characters in the corpus\n",
    "3. Compute the frequency of a pair of characters or character sequences in corpus\n",
    "4. Merge the most frequent pair in corpus\n",
    "5. Save the best pair to the vocabulary\n",
    "6. Repeat steps 3 to 5 for a certain number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPiece is similar to BPE techniques expect the way the new token is added to the vocabulary. BPE considers the token with most frequent occurring pair of symbols to merge into the vocabulary. While WordPiece considers the frequency of individual symbols also and based on below count it merges into the vocabulary.\n",
    "- Count (x, y) = frequency of (x, y) / frequency (x) * frequency (y)\n",
    "- The pair of symbols with maximum count will be considered to merge into vocabulary. So it allows rare tokens to be included into vocabulary as compared to BPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii) Unigram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to BPE or WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each symbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and the most common substrings. Unigram is not used directly for any of the models in the transformers, but it’s used in conjunction with SentencePiece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv) SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing which treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.\n",
    "- All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words.To solve this problem more generally Sentence Piece was intoduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
